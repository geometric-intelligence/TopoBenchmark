_target_: topobenchmark.optimizer.TBOptimizer
# Full compatibility with all available torch optimizers and schedulers

optimizer_id: Adam # torch id of the optimizer
# optimizer params
parameters:
  lr: 0.001
  weight_decay: 0.0

scheduler: # Comment or delete these lines to disable the scheduler
  scheduler_id: StepLR # torch id of the scheduler. Set to null to disable.
  # scheduler params
  scheduler_params:
    step_size: 50
    gamma: 0.5
