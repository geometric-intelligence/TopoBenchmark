{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define download functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import parse_qs, urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# Function to extract file ID from Google Drive URL\n",
    "def get_file_id_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts the file ID from a Google Drive file URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The Google Drive file URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The file ID extracted from the URL.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the provided URL is not a valid Google Drive file URL.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    if \"id\" in query_params:  # Case 1: URL format contains '?id='\n",
    "        file_id = query_params[\"id\"][0]\n",
    "    elif \"file/d/\" in parsed_url.path:  # Case 2: URL format contains '/file/d/'\n",
    "        file_id = parsed_url.path.split(\"/\")[3]\n",
    "    else:\n",
    "        raise ValueError(\"The provided URL is not a valid Google Drive file URL.\")\n",
    "    return file_id\n",
    "\n",
    "\n",
    "# Function to download file from Google Drive\n",
    "def download_file_from_drive(\n",
    "    file_link, path_to_save, dataset_name, file_format=\"tar.gz\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads a file from a Google Drive link and saves it to the specified path.\n",
    "\n",
    "    Args:\n",
    "        file_link (str): The Google Drive link of the file to download.\n",
    "        path_to_save (str): The path where the downloaded file will be saved.\n",
    "        dataset_name (str): The name of the dataset.\n",
    "        file_format (str, optional): The format of the downloaded file. Defaults to \"tar.gz\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_id = get_file_id_from_url(file_link)\n",
    "\n",
    "    download_link = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    response = requests.get(download_link)\n",
    "\n",
    "    output_path = f\"{path_to_save}/{dataset_name}.{file_format}\"\n",
    "    if response.status_code == 200:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        print(\"Failed to download the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the data load function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "\n",
    "def load_us_county_demos(path, year=2012):\n",
    "    edges_df = pd.read_csv(f\"{path}/county_graph.csv\")\n",
    "    stat = pd.read_csv(f\"{path}/county_stats_{year}.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "    keep_cols = [\n",
    "        \"FIPS\",\n",
    "        \"DEM\",\n",
    "        \"GOP\",\n",
    "        \"MedianIncome\",\n",
    "        \"MigraRate\",\n",
    "        \"BirthRate\",\n",
    "        \"DeathRate\",\n",
    "        \"BachelorRate\",\n",
    "        \"UnemploymentRate\",\n",
    "    ]\n",
    "    # Drop rows with missing values\n",
    "    stat = stat[keep_cols].dropna()\n",
    "\n",
    "    # Delete edges that are not present in stat df\n",
    "    unique_fips = stat[\"FIPS\"].unique()\n",
    "\n",
    "    src_ = edges_df[\"SRC\"].apply(lambda x: x in unique_fips)\n",
    "    dst_ = edges_df[\"DST\"].apply(lambda x: x in unique_fips)\n",
    "\n",
    "    edges_df = edges_df[src_ & dst_]\n",
    "\n",
    "    # Remove rows from stat df where edges_df['SRC'] or edges_df['DST'] are not present\n",
    "    stat = stat[stat[\"FIPS\"].isin(edges_df[\"SRC\"]) & stat[\"FIPS\"].isin(edges_df[\"DST\"])]\n",
    "    stat = stat.reset_index(drop=True)\n",
    "\n",
    "    # Remove rows where SRC == DST\n",
    "    edges_df = edges_df[edges_df[\"SRC\"] != edges_df[\"DST\"]]\n",
    "\n",
    "    # Get torch_geometric edge_index format\n",
    "    edge_index = torch.tensor(\n",
    "        np.stack([edges_df[\"SRC\"].to_numpy(), edges_df[\"DST\"].to_numpy()])\n",
    "    )\n",
    "\n",
    "    # Make edge_index undirected\n",
    "    edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
    "\n",
    "    # Convert edge_index back to pandas DataFrame\n",
    "    edges_df = pd.DataFrame(edge_index.numpy().T, columns=[\"SRC\", \"DST\"])\n",
    "\n",
    "    del edge_index\n",
    "\n",
    "    # Map stat['FIPS'].unique() to [0, ..., num_nodes]\n",
    "    fips_map = {fips: i for i, fips in enumerate(stat[\"FIPS\"].unique())}\n",
    "    stat[\"FIPS\"] = stat[\"FIPS\"].map(fips_map)\n",
    "\n",
    "    # Map edges_df['SRC'] and edges_df['DST'] to [0, ..., num_nodes]\n",
    "    edges_df[\"SRC\"] = edges_df[\"SRC\"].map(fips_map)\n",
    "    edges_df[\"DST\"] = edges_df[\"DST\"].map(fips_map)\n",
    "\n",
    "    # Get torch_geometric edge_index format\n",
    "    edge_index = torch.tensor(\n",
    "        np.stack([edges_df[\"SRC\"].to_numpy(), edges_df[\"DST\"].to_numpy()])\n",
    "    )\n",
    "\n",
    "    # Remove isolated nodes (Note: this function maps the nodes to [0, ..., num_nodes] automatically)\n",
    "    edge_index, _, mask = torch_geometric.utils.remove_isolated_nodes(edge_index)\n",
    "\n",
    "    # Conver mask to index\n",
    "    index = np.arange(mask.size(0))[mask]\n",
    "    stat = stat.iloc[index]\n",
    "    stat = stat.reset_index(drop=True)\n",
    "\n",
    "    # Get new values for FIPS from current index\n",
    "    # To understand why please print stat.iloc[[516, 517, 518, 519, 520]] for 2012 year\n",
    "    # Basically the FIPS values has been shifted\n",
    "    stat[\"FIPS\"] = stat.reset_index()[\"index\"]\n",
    "\n",
    "    # Create Election variable\n",
    "    stat[\"Election\"] = (stat[\"DEM\"] - stat[\"GOP\"]) / (stat[\"DEM\"] + stat[\"GOP\"])\n",
    "\n",
    "    # Drop DEM and GOP columns and FIPS\n",
    "    stat = stat.drop(columns=[\"DEM\", \"GOP\", \"FIPS\"])\n",
    "\n",
    "    # Prediction col\n",
    "    y_col = \"Election\"  # TODO: Define through config file\n",
    "    x_col = list(set(stat.columns).difference(set([y_col])))\n",
    "\n",
    "    stat[\"MedianIncome\"] = (\n",
    "        stat[\"MedianIncome\"]\n",
    "        .apply(lambda x: x.replace(\",\", \"\"))\n",
    "        .to_numpy()\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "    x = stat[x_col].to_numpy()\n",
    "    y = stat[y_col].to_numpy()\n",
    "\n",
    "    data = torch_geometric.data.Data(x=x, y=y, edge_index=edge_index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the dataset class\n",
    "\n",
    "The dataset class inherits InMemoryDataset (torch_geometric).\n",
    "\n",
    "Next it is esential to overwrite three methods: __init__, download, process\n",
    "\n",
    "As well as a number of properties: raw_dir, processed_dir, raw_file_names, processed_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from collections.abc import Callable\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.io import fs\n",
    "\n",
    "from topobenchmarkx.io.load.cornel_dataset import load_us_county_demos\n",
    "from topobenchmarkx.io.load.download_utils import download_file_from_drive\n",
    "from topobenchmarkx.io.load.split_utils import random_splitting\n",
    "\n",
    "\n",
    "class USCountyDemosDataset(InMemoryDataset):\n",
    "    r\"\"\"\n",
    "    Dataset class for US County Demographics dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset will be saved.\n",
    "        name (str): Name of the dataset.\n",
    "        parameters (DictConfig): Configuration parameters for the dataset.\n",
    "        transform (Optional[Callable]): A function/transform that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a transformed version.\n",
    "            The transform function is applied to the loaded data before saving it.\n",
    "        pre_transform (Optional[Callable]): A function/transform that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a transformed version.\n",
    "            The pre_transform function is applied to the data before the transform\n",
    "            function is applied.\n",
    "        pre_filter (Optional[Callable]): A function that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a boolean value\n",
    "            indicating whether the data object should be included in the dataset.\n",
    "        force_reload (bool): If set to True, the dataset will be re-downloaded\n",
    "            even if it already exists on disk. (default: True)\n",
    "        use_node_attr (bool): If set to True, the node attributes will be included\n",
    "            in the dataset. (default: False)\n",
    "        use_edge_attr (bool): If set to True, the edge attributes will be included\n",
    "            in the dataset. (default: False)\n",
    "\n",
    "    Attributes:\n",
    "        URLS (dict): Dictionary containing the URLs for downloading the dataset.\n",
    "        FILE_FORMAT (dict): Dictionary containing the file formats for the dataset.\n",
    "        RAW_FILE_NAMES (dict): Dictionary containing the raw file names for the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    URLS = {\n",
    "        \"US-county-demos\": \"https://drive.google.com/file/d/1FNF_LbByhYNICPNdT6tMaJI9FxuSvvLK/view?usp=sharing\",\n",
    "    }\n",
    "\n",
    "    FILE_FORMAT = {\n",
    "        \"US-county-demos\": \"zip\",\n",
    "    }\n",
    "\n",
    "    RAW_FILE_NAMES = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        parameters: DictConfig,\n",
    "        transform: Callable | None = None,\n",
    "        pre_transform: Callable | None = None,\n",
    "        pre_filter: Callable | None = None,\n",
    "        force_reload: bool = True,\n",
    "    ) -> None:\n",
    "        # Assign the class variables that would be needed for steps 1, 2, 4, and 3\n",
    "        self.name = name.replace(\"_\", \"-\")\n",
    "        self.parameters = parameters\n",
    "\n",
    "        # Static, do not modify\n",
    "        # --------------------------------------------------------\n",
    "        super().__init__(\n",
    "            root, transform, pre_transform, pre_filter, force_reload=force_reload\n",
    "        )\n",
    "\n",
    "        # Logic that should be modified while adding new dataset:\n",
    "        # --------------------------------------------------------\n",
    "        # Step 3:Load the processed data\n",
    "        # After the data has been downloaded from source\n",
    "        # Then preprocessed to obtain x,y and saved into processed folder\n",
    "        # We can now load the processed data from processed folder\n",
    "\n",
    "        # Load the processed data\n",
    "        data, _, _ = fs.torch_load(self.processed_paths[0])\n",
    "\n",
    "        # Map the loaded data into\n",
    "        data = Data.from_dict(data)\n",
    "\n",
    "        # Step 4: Create the splits and upload desired fold\n",
    "        splits = random_splitting(data.y, parameters=self.parameters)\n",
    "        # Assign train val test masks to the graph\n",
    "        data.train_mask = torch.from_numpy(splits[\"train\"])\n",
    "        data.val_mask = torch.from_numpy(splits[\"valid\"])\n",
    "        data.test_mask = torch.from_numpy(splits[\"test\"])\n",
    "\n",
    "        # Assign data object to self.data, to make it be prodessed by Dataset class\n",
    "        self.data = data\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"processed\")\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list[str]:\n",
    "        names = [\"\", \"_2012\"]\n",
    "        return [f\"{self.name}_{name}.txt\" for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return \"data.pt\"\n",
    "\n",
    "    def download(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the dataset from the specified URL and saves it to the raw directory.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset URL is not found.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Download data from the source\n",
    "        self.url = self.URLS[self.name]\n",
    "        self.file_format = self.FILE_FORMAT[self.name]\n",
    "\n",
    "        download_file_from_drive(\n",
    "            file_link=self.url,\n",
    "            path_to_save=self.raw_dir,\n",
    "            dataset_name=self.name,\n",
    "            file_format=self.file_format,\n",
    "        )\n",
    "\n",
    "        # Extract the downloaded file if it is compressed\n",
    "        fs.cp(\n",
    "            f\"{self.raw_dir}/{self.name}.{self.file_format}\", self.raw_dir, extract=True\n",
    "        )\n",
    "\n",
    "        # Move the etracted files to the datasets/domain/dataset_name/raw/ directory\n",
    "        for filename in fs.ls(osp.join(self.raw_dir, self.name)):\n",
    "            fs.mv(filename, osp.join(self.raw_dir, osp.basename(filename)))\n",
    "        fs.rm(osp.join(self.raw_dir, self.name))\n",
    "\n",
    "        # Delete also f'{self.raw_dir}/{self.name}.{self.file_format}'\n",
    "        fs.rm(f\"{self.raw_dir}/{self.name}.{self.file_format}\")\n",
    "\n",
    "    def process(self) -> None:\n",
    "        \"\"\"\n",
    "        Process the data for the dataset.\n",
    "\n",
    "        This method loads the US county demographics data, applies any pre-processing transformations if specified,\n",
    "        and saves the processed data to the appropriate location.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        data = load_us_county_demos(self.raw_dir, year=self.parameters.year)\n",
    "\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}({len(self)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterophilic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def hetero_load(name, path=\"./data/hetero_data\"):\n",
    "    file_name = f\"{name}.npz\"\n",
    "\n",
    "    data = np.load(os.path.join(path, file_name))\n",
    "\n",
    "    x = torch.tensor(data[\"node_features\"])\n",
    "    y = torch.tensor(data[\"node_labels\"])\n",
    "    edge_index = torch.tensor(data[\"edges\"]).T\n",
    "\n",
    "    # Make edge_index undirected\n",
    "    edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
    "\n",
    "    # Remove self-loops\n",
    "    edge_index, _ = torch_geometric.utils.remove_self_loops(edge_index)\n",
    "\n",
    "    data = torch_geometric.data.Data(x=x, y=y, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "\n",
    "def download_hetero_datasets(name, path):\n",
    "    url = \"https://github.com/OpenGSL/HeterophilousDatasets/raw/main/data/\"\n",
    "    name = f\"{name}.npz\"\n",
    "    try:\n",
    "        print(f\"Downloading {name}\")\n",
    "        path2save = os.path.join(path, name)\n",
    "        urllib.request.urlretrieve(url + name, path2save)\n",
    "        print(\"Done!\")\n",
    "    except:\n",
    "        raise Exception(\n",
    "            \"\"\"Download failed! Make sure you have stable Internet connection and enter the right name\"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "from collections.abc import Callable\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "\n",
    "from topobenchmarkx.io.load.us_county_demos import load_us_county_demos\n",
    "\n",
    "\n",
    "class HeteroDataset(InMemoryDataset):\n",
    "    r\"\"\"\n",
    "    Dataset class for US County Demographics dataset.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset will be saved.\n",
    "        name (str): Name of the dataset.\n",
    "        parameters (DictConfig): Configuration parameters for the dataset.\n",
    "        transform (Optional[Callable]): A function/transform that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a transformed version.\n",
    "            The transform function is applied to the loaded data before saving it.\n",
    "        pre_transform (Optional[Callable]): A function/transform that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a transformed version.\n",
    "            The pre_transform function is applied to the data before the transform\n",
    "            function is applied.\n",
    "        pre_filter (Optional[Callable]): A function that takes in an\n",
    "            `torch_geometric.data.Data` object and returns a boolean value\n",
    "            indicating whether the data object should be included in the dataset.\n",
    "        force_reload (bool): If set to True, the dataset will be re-downloaded\n",
    "            even if it already exists on disk. (default: True)\n",
    "        use_node_attr (bool): If set to True, the node attributes will be included\n",
    "            in the dataset. (default: False)\n",
    "        use_edge_attr (bool): If set to True, the edge attributes will be included\n",
    "            in the dataset. (default: False)\n",
    "\n",
    "    Attributes:\n",
    "        URLS (dict): Dictionary containing the URLs for downloading the dataset.\n",
    "        FILE_FORMAT (dict): Dictionary containing the file formats for the dataset.\n",
    "        RAW_FILE_NAMES (dict): Dictionary containing the raw file names for the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    RAW_FILE_NAMES = {}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        name: str,\n",
    "        parameters: DictConfig,\n",
    "        transform: Callable | None = None,\n",
    "        pre_transform: Callable | None = None,\n",
    "        pre_filter: Callable | None = None,\n",
    "        force_reload: bool = True,\n",
    "        use_node_attr: bool = False,\n",
    "        use_edge_attr: bool = False,\n",
    "    ) -> None:\n",
    "        self.name = name  # .replace(\"_\", \"-\")\n",
    "        self.parameters = parameters\n",
    "        super().__init__(\n",
    "            root, transform, pre_transform, pre_filter, force_reload=force_reload\n",
    "        )\n",
    "\n",
    "        # Step 3:Load the processed data\n",
    "        # After the data has been downloaded from source\n",
    "        # Then preprocessed to obtain x,y and saved into processed folder\n",
    "        # We can now load the processed data from processed folder\n",
    "\n",
    "        # Load the processed data\n",
    "        data, _, _ = fs.torch_load(self.processed_paths[0])\n",
    "\n",
    "        # Map the loaded data into\n",
    "        data = Data.from_dict(data)\n",
    "\n",
    "        # Step 5: Create the splits and upload desired fold\n",
    "        splits = random_splitting(data.y, parameters=self.parameters)\n",
    "        # Assign train val test masks to the graph\n",
    "        data.train_mask = torch.from_numpy(splits[\"train\"])\n",
    "        data.val_mask = torch.from_numpy(splits[\"valid\"])\n",
    "        data.test_mask = torch.from_numpy(splits[\"test\"])\n",
    "\n",
    "        # Assign data object to self.data, to make it be prodessed by Dataset class\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "    # Do not forget to take care of properties\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, \"processed\")\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return \"data.pt\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> list[str]:\n",
    "        \"\"\"Spefify the downloaded raw fine name\"\"\"\n",
    "        return [f\"{self.name}.npz\"]\n",
    "\n",
    "    def download(self) -> None:\n",
    "        \"\"\"\n",
    "        Downloads the dataset from the specified URL and saves it to the raw directory.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the dataset URL is not found.\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Download data from the source\n",
    "        download_hetero_datasets(name=self.name, path=self.raw_dir)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        \"\"\"\n",
    "        Process the data for the dataset.\n",
    "\n",
    "        This method loads the US county demographics data, applies any pre-processing transformations if specified,\n",
    "        and saves the processed data to the appropriate location.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        data = hetero_load(name=self.name, path=self.raw_dir)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}()\"\n",
    "\n",
    "\n",
    "data_dir = \"/home/lev/projects/TopoBenchmarkX/datasets\"\n",
    "data_domain = \"graph\"\n",
    "data_type = \"heterophilic\"\n",
    "data_name = \"amazon_ratings\"\n",
    "\n",
    "data_dir = f\"{data_dir}/{data_domain}/{data_type}\"\n",
    "\n",
    "parameters = {\n",
    "    \"split_type\": \"random\",\n",
    "    \"k\": 10,\n",
    "    \"train_prop\": 0.5,\n",
    "    \"data_seed\": 0,\n",
    "    \"data_split_dir\": f\"/home/lev/projects/TopoBenchmarkX/datasets/data_splits/{data_name}\",\n",
    "}\n",
    "\n",
    "dataset = HeteroDataset(\n",
    "    name=data_name, root=data_dir, parameters=parameters, force_reload=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
